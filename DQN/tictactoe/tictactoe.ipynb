{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "import utils_tictactoe\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "tf.random.set_seed(utils_tictactoe.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 1e-3              # learning rate  \n",
    "NUM_STEPS_FOR_UPDATE = 3  # perform a learning update every C time steps\n",
    "X = \"X\"\n",
    "O = \"O\"\n",
    "EMPTY = None\n",
    "BOARD_SIZE = 3\n",
    "ALL_ACTIONS = [(r, c) for r in range(BOARD_SIZE) for c in range(BOARD_SIZE)]\n",
    "board_size = 3\n",
    "state_size = 9\n",
    "action_size = 9\n",
    "# Store experiences as named tuples\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tictactoe():\n",
    "\n",
    "    def __init__(self, initial=[[EMPTY for _ in range(board_size)] for _ in range(board_size)]):\n",
    "        \"\"\"\n",
    "        Initialize game board.\n",
    "        board: use 2d list to represent the state\n",
    "        first player is 0, second is 1\n",
    "        \"\"\"\n",
    "        self.board = copy.deepcopy(initial)\n",
    "        self.player = 0\n",
    "        self.winner = None\n",
    "        self.left = 9  # EMPTY grid left in game board\n",
    "\n",
    "    @classmethod\n",
    "    def available_actions(cls, board):\n",
    "        \"\"\"\n",
    "        tictactoe.available_actions(board) takes a `board` list as input\n",
    "        and returns all of the available actions `(i, j)` in that state.\n",
    "\n",
    "        Action `(i, j)` represents the action of make a move in raw_i,column_j\n",
    "        \"\"\"\n",
    "        actions = set()\n",
    "        for i in range(3) :\n",
    "            for j in range(3) :\n",
    "                if board[i][j] == EMPTY :\n",
    "                    actions.add((i,j))\n",
    "        return actions\n",
    "\n",
    "    @classmethod\n",
    "    def other_player(cls, player):\n",
    "        \"\"\"\n",
    "        tictactoe.other_player(player) returns the player that is not\n",
    "        `player`. Assumes `player` is either 0 or 1.\n",
    "        \"\"\"\n",
    "        return 0 if player == 1 else 1\n",
    "\n",
    "    def switch_player(self):\n",
    "        \"\"\"\n",
    "        Switch the current player to the other player.\n",
    "        \"\"\"\n",
    "        self.player = tictactoe.other_player(self.player)\n",
    "\n",
    "    def now_player_sign(self, player):\n",
    "        if player == 0:\n",
    "            return X\n",
    "        else:\n",
    "            return O\n",
    "        \n",
    "    # check for winner after each action taken by players\n",
    "    def check_winner(self, action):\n",
    "        r,c = action\n",
    "        base_sign = self.board[r][c]  # get current player's sign\n",
    "        winner_reward = [None, 0]  # record winner and reward\n",
    "\n",
    "        # if winner is none and base_sign is current player's sign, we need to check winner\n",
    "        def winner(base_sign, k):\n",
    "            if winner_reward[0] is None and k:\n",
    "                if base_sign == X:\n",
    "                    winner_reward[0] = 0\n",
    "                else:\n",
    "                    winner_reward[0] = 1\n",
    "        \n",
    "        # whoever the winner is, we give player that win or block rival's win 1 reward\n",
    "        def get_reward(base_sign, r, c, k):\n",
    "            # check for rows\n",
    "            if self.board[r][0] == base_sign and base_sign == self.board[r][1] and base_sign == self.board[r][2]:\n",
    "                winner_reward[1] = 1\n",
    "                winner(base_sign, k)\n",
    "            # check for columns\n",
    "            if self.board[0][c] == base_sign and base_sign == self.board[1][c] and base_sign == self.board[2][c]:\n",
    "                winner_reward[1] = 1\n",
    "                winner(base_sign, k)\n",
    "            # check for first diagnal\n",
    "            if r == c:\n",
    "                if self.board[0][0] == base_sign and self.board[1][1] == base_sign and self.board[2][2] == base_sign :\n",
    "                    winner_reward[1] = 1\n",
    "                    winner(base_sign, k)\n",
    "            # check for second diagnal\n",
    "            if r+c == 2:\n",
    "                if self.board[0][2] == base_sign and self.board[1][1] == base_sign and self.board[2][0] == base_sign :\n",
    "                    winner_reward[1] = 1\n",
    "                    winner(base_sign, k)\n",
    "        # base_sign is current player's sign, we need check winner and reward, k=1\n",
    "        get_reward(base_sign, r, c, 1)\n",
    "\n",
    "        # base_sign is not current player's sign, we need check only the reward, k=0\n",
    "        base_sign = self.now_player_sign(self.player)\n",
    "        self.board[r][c] = base_sign\n",
    "        get_reward(base_sign, r, c, 0)\n",
    "        self.board[r][c] = self.now_player_sign(1-self.player)\n",
    "        return winner_reward\n",
    "    \n",
    "    def board_to_state(self):\n",
    "        \"\"\"\n",
    "        neuron network can only take numbers as input. \n",
    "        first player is X and 1; second is O and -1; empty is 0\n",
    "        \"\"\"\n",
    "        translation = {X:1, O:-1, EMPTY:0}\n",
    "        state = tuple([tuple([translation[j] for j in row])for row in self.board])\n",
    "        return state\n",
    "    \n",
    "    def move(self, action):\n",
    "        \"\"\"\n",
    "        Make the move `action` for the current player.\n",
    "        `action` must be a tuple `(i, j)`.\n",
    "        \"\"\"\n",
    "        raw, column = action\n",
    "\n",
    "        # Check for errors\n",
    "        if self.winner is not None:\n",
    "            raise Exception(\"Game already won\")\n",
    "        elif raw < 0 or raw >= 3 or column < 0 or column >= 3:\n",
    "            raise Exception(\"Invalid move\")\n",
    "\n",
    "        # Check if ai made a move on the place already made a move\n",
    "        # not happen if we apply a mask on the output of nn \n",
    "        if self.board[raw][column] is not None:\n",
    "            print(\"error:ai made an invalid move\")\n",
    "            return self.board_to_state(), 0, 1\n",
    "        \n",
    "        # Update board\n",
    "        self.board[raw][column] = self.now_player_sign(self.player)\n",
    "        self.switch_player()\n",
    "        self.left -= 1\n",
    "\n",
    "        # check for winner and get reward\n",
    "        # however, we can simply give 2 for winner, and -2 for loser\n",
    "        # don't need to use this function to calculate the reward \n",
    "        self.winner, _ = self.check_winner(action)\n",
    "\n",
    "        # move function return state, reward, done(1 for terminal, 0 for not)\n",
    "        if self.winner is not None:\n",
    "            # second last move must be made by loser\n",
    "            return self.board_to_state(), -2, 1\n",
    "        elif self.left == 0:\n",
    "            # we use 2 to represent tie\n",
    "            self.winner = 2\n",
    "            return self.board_to_state(), 0, 1\n",
    "        # game continuing\n",
    "        else:\n",
    "            return self.board_to_state(), 0, 0 \n",
    "    \n",
    "    # print the board\n",
    "    def render(self):\n",
    "        print()\n",
    "        print(\"board:\")\n",
    "        print(\"   0 1 2\")\n",
    "        for i in range(3):\n",
    "            print(i,end=\"  \")\n",
    "            for j in self.board[i]:\n",
    "                print(j if j is not None else '-',end=\" \")\n",
    "            print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Q-Network\n",
    "# 64-64 is enough\n",
    "q_network = Sequential([\n",
    "    ### START CODE HERE ### \n",
    "    Input(state_size),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(action_size, activation='linear')\n",
    "    ### END CODE HERE ### \n",
    "    ])\n",
    "\n",
    "# Create the target Q^-Network\n",
    "target_q_network = Sequential([\n",
    "    ### START CODE HERE ### \n",
    "    Input(state_size),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(action_size, activation='linear')\n",
    "    ### END CODE HERE ###\n",
    "    ])\n",
    "\n",
    "### START CODE HERE ### \n",
    "optimizer = Adam(ALPHA)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "in states, 1 and -1 is places already made moves, so we replace them with -100\n",
    "to avoid making repetitive move\n",
    "'''\n",
    "def get_mask(states):\n",
    "    mask = tf.where(tf.equal(states, 1.0), -100.0, states)\n",
    "    mask = tf.where(tf.equal(mask, -1.0), -100.0, mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    \"\"\" \n",
    "    Calculates the loss.\n",
    "    \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "      q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
    "      target_q_network: (tf.keras.Sequential) Karas model for predicting the targets\n",
    "          \n",
    "    Returns:\n",
    "      loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
    "            the y targets and the Q(s,a) values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack the mini-batch of experience tuples. all of them are tf.tensor\n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "    \n",
    "    # Compute max Q^(s,a)\n",
    "    mask = get_mask(next_states)\n",
    "    max_qsa = tf.reduce_max(target_q_network(next_states) + mask, axis=-1)  # (64,)tensor\n",
    "\n",
    "    # Set y = R if episode terminates, otherwise set y = R + γ max Q^(s,a).\n",
    "    ### START CODE HERE ### \n",
    "    y_targets = done_vals * rewards + (1 - done_vals) * (rewards + gamma * max_qsa)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Get the q_values\n",
    "    q_values = q_network(states)\n",
    "    q_values = tf.gather(q_values, actions, axis=1, batch_dims=1)  # (64,)tensor\n",
    "    # Compute the loss\n",
    "    ### START CODE HERE ### \n",
    "    loss = MSE(q_values, y_targets) \n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def agent_learn(experiences, gamma):\n",
    "    \"\"\"\n",
    "    Updates the weights of the Q networks.\n",
    "    \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(experiences, gamma, q_network, target_q_network)\n",
    "\n",
    "    # Get the gradients of the loss with respect to the weights.\n",
    "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "    \n",
    "    # Update the weights of the q_network.\n",
    "    optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "\n",
    "    # update the weights of target q_network\n",
    "    utils_tictactoe.update_target_network(q_network, target_q_network)\n",
    "\n",
    "    return loss  # to report the loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the 2d list to (1, 9) tensor, suitable for nn input\n",
    "def state_to_input(state):\n",
    "    state_flat = np.array(state).flatten()\n",
    "    return tf.convert_to_tensor(state_flat, dtype=tf.float32)[tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "num_rounds = 20000\n",
    "\n",
    "total_loss_history = []\n",
    "\n",
    "epsilon = 1    # initial ε value for ε-greedy policy\n",
    "\n",
    "# Create a memory buffer D with capacity N\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "# Set the target network weights equal to the Q-Network weights\n",
    "target_q_network.set_weights(q_network.get_weights())\n",
    "\n",
    "for i in range(num_rounds):\n",
    "    \n",
    "    # Reset the environment to the initial state and get the initial state\n",
    "    game = tictactoe()\n",
    "    last = {\n",
    "            0: {\"state\": None, \"action\": None},\n",
    "            1: {\"state\": None, \"action\": None},\n",
    "        }\n",
    "\n",
    "    for t in range(9):\n",
    "        \n",
    "        state = game.board_to_state() # 2 dimension tuple\n",
    "        # From the current state S choose an action A using an ε-greedy policy\n",
    "        # state needs to be the right shape for the q_network\n",
    "        input_qn = state_to_input(state) # (1,9) tf tensor\n",
    "        q_values = q_network(input_qn) # (1,9) tf tensor\n",
    "        action = utils_tictactoe.get_action(q_values, input_qn, epsilon) # tuple(i, j)\n",
    "        last[game.player][\"state\"] = state\n",
    "        last[game.player][\"action\"] = action\n",
    "\n",
    "        # Make move\n",
    "        next_state, reward, done = game.move(action)\n",
    "\n",
    "        state = last[game.player][\"state\"]\n",
    "        action = last[game.player][\"action\"]\n",
    "\n",
    "\n",
    "        # Store experience tuple (S,A,R,S') in the memory buffer.\n",
    "        # We store the done variable as well for convenience.\n",
    "        if t != 0:\n",
    "            memory_buffer.append(experience(state, action, reward, next_state, done))\n",
    "        if game.winner is not None:\n",
    "            memory_buffer.append(experience(last[1-game.player][\"state\"], last[1-game.player][\"action\"], -reward, next_state, done))\n",
    "            \n",
    "\n",
    "        # Only update the network every NUM_STEPS_FOR_UPDATE time steps.\n",
    "        update = utils_tictactoe.check_update_conditions(t, NUM_STEPS_FOR_UPDATE, memory_buffer)\n",
    "        \n",
    "        if update:\n",
    "            # Sample random mini-batch of experience tuples (S,A,R,S') from Deque\n",
    "            experiences = utils_tictactoe.get_experiences(memory_buffer)\n",
    "            \n",
    "            # Set the y targets, perform a gradient descent step,\n",
    "            # and update the network weights.\n",
    "            loss = agent_learn(experiences, GAMMA)\n",
    "            total_loss_history.append(loss)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    # Update the ε value\n",
    "    epsilon = utils_tictactoe.get_new_eps(epsilon)\n",
    "    av_latest_loss = np.mean(total_loss_history[-1000:])\n",
    "    print(f\"\\rEpisode {i+1} | Total loss average: {av_latest_loss:.2f}\", end=\"\")\n",
    "\n",
    "    if (i+1) % 1000 == 0:\n",
    "        print(f\"\\rEpisode {i+1} | Total loss average: {av_latest_loss:.2f}\")\n",
    "    \n",
    "    if av_latest_loss <= 0.05:\n",
    "        break\n",
    "\n",
    "        \n",
    "tot_time = time.time() - start\n",
    "\n",
    "print(f\"\\nTotal Runtime: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool to print last game to debug\n",
    "def print_last_game(buffer):\n",
    "    for e in list(buffer)[-7:]:\n",
    "        for i in range(3):\n",
    "            print(f'{e.state[i]}   ->   {e.next_state[i]}')\n",
    "        print(f'action: {e.action}, reward:{e.reward}, done: {e.done}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(human_player=None):\n",
    "    \"\"\"\n",
    "    Play human game against the AI and also store the experience tuple (S,A,R,S') in the memory buffer.\n",
    "    `human_player` can be set to 0 or 1 to specify whether\n",
    "    human player moves first or second.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If no player order set, choose human's order randomly\n",
    "    if human_player is None:\n",
    "        human_player = random.randint(0, 1)\n",
    "\n",
    "    # Create new game\n",
    "    game = tictactoe()\n",
    "    last = {\n",
    "        0: {\"state\": None, \"action\": None},\n",
    "        1: {\"state\": None, \"action\": None},\n",
    "    }\n",
    "\n",
    "    # Game loop\n",
    "    while True:\n",
    "\n",
    "        # Print contents of board\n",
    "        game.render()\n",
    "\n",
    "        # Compute available actions\n",
    "        available_actions = tictactoe.available_actions(game.board) #tuple(i, j)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Let human make a move\n",
    "        if game.player == human_player:\n",
    "            print(\"Your Turn\")\n",
    "            while True:\n",
    "                # only int input is legal, otherwise will exit play\n",
    "                row = int(input(\"Choose Row: \"))\n",
    "                column = int(input(\"Choose Column: \"))\n",
    "                if (row, column) in available_actions:\n",
    "                    action = (row, column)\n",
    "                    break\n",
    "                print(\"Invalid move, try again.\")\n",
    "\n",
    "        # Have AI make a move\n",
    "        else:\n",
    "            print(\"AI's Turn\")\n",
    "            state = game.board_to_state() # 2 dimension tuple\n",
    "            input_qn = state_to_input(state) # (1, 9) tf tensor\n",
    "            q_values = q_network(input_qn) # (1, 9) tf tensor\n",
    "            action = utils_tictactoe.get_action(q_values, input_qn) # tuple(i, j)\n",
    "            row, column = action\n",
    "            print(f\"AI chose to move row {row}, column {column}.\")\n",
    "\n",
    "        state = game.board_to_state()\n",
    "        last[game.player][\"state\"] = state\n",
    "        last[game.player][\"action\"] = action\n",
    "\n",
    "        # Make move\n",
    "        next_state, reward, done = game.move((row, column))\n",
    "\n",
    "        state = last[game.player][\"state\"]\n",
    "        action = last[game.player][\"action\"]\n",
    "\n",
    "        if t != 0:\n",
    "            memory_buffer.append(experience(state, action, reward, next_state, done))\n",
    "        if game.winner is not None:\n",
    "            memory_buffer.append(experience(last[1-game.player][\"state\"], last[1-game.player][\"action\"], -reward, next_state, done))\n",
    "        # Check for winner\n",
    "        if done:\n",
    "            game.render()\n",
    "            print()\n",
    "            print(\"GAME OVER\")\n",
    "            if game.winner == human_player:\n",
    "                print(f\"Winner is human\")\n",
    "            elif game.winner == 1-human_player:\n",
    "                print(f\"Winner is AI\")\n",
    "            else:\n",
    "                print(\"Tie\")\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    play(0)\n",
    "for i in range(3):\n",
    "    play(1)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_last_game(memory_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "q_network.save('q_model_tictactoe', save_format='tf')\n",
    "target_q_network.save('target_q_model_tictactoe', save_format='tf')\n",
    "\n",
    "# load model\n",
    "#q_network = tf.keras.models.load_model('q_model_tictactoe')\n",
    "#target_q_network = tf.keras.models.load_model('target_q_model_tictactoe')\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pseudo_matlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
