{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Import Packages\n",
    "\n",
    "We'll make use of the following packages:\n",
    "- `numpy` is a package for scientific computing in python.\n",
    "- `deque` will be our data structure for our memory buffer.\n",
    "- `namedtuple` will be used to store the experience tuples.\n",
    "- The `gym` toolkit is a collection of environments that can be used to test reinforcement learning algorithms. We should note that in this notebook we are using `gym` version `0.24.0`.\n",
    "- `PIL.Image` and `pyvirtualdisplay` are needed to render the Lunar Lander environment.\n",
    "- We will use several modules from the `tensorflow.keras` framework for building deep learning models.\n",
    "- `utils` is a module that contains helper functions for this assignment. You do not need to modify the code in this file.\n",
    "\n",
    "Run the cell below to import all the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYbOPKRtfQOr"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - Hyperparameters\n",
    "\n",
    "Run the cell below to set the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.9              # discount factor\n",
    "ALPHA = 1e-3              # learning rate  \n",
    "NUM_STEPS_FOR_UPDATE = 5  # perform a learning update every C time steps\n",
    "X = \"X\"\n",
    "O = \"O\"\n",
    "EMPTY = None\n",
    "board_size = 5\n",
    "state_size = 25\n",
    "action_size = 25\n",
    "# Store experiences as named tuples\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "\n",
    "MINIBATCH_SIZE = 64   # mini-batch size\n",
    "TAU = 1e-3            # soft update parameter\n",
    "E_DECAY = 0.999       # ε decay rate for ε-greedy policy\n",
    "E_MIN = 0.05          # minimum ε value for ε-greedy policy\n",
    "BOARD_SIZE = 5\n",
    "ALL_ACTIONS = [(r, c) for r in range(BOARD_SIZE) for c in range(BOARD_SIZE)]\n",
    "SELF_RACIO = 0.4\n",
    "# Set the random seed for TensorFlow\n",
    "SEED = 0              # seed for pseudo-random number generator\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3 - The Five_tigers game\n",
    "\n",
    "<a name=\"3.1\"></a>\n",
    "### 3.1 Action Space\n",
    "\n",
    "(i, j) left on the board (using mask to avoid invalid move)\n",
    "\n",
    "<a name=\"3.2\"></a>\n",
    "### 3.2 State\n",
    "\n",
    "(i, j) steps already on the board\n",
    "* 1 for self move\n",
    "* 0 for empty\n",
    "* -1 for rival move\n",
    "* express as a 25 length vector\n",
    "<a name=\"3.3\"></a>\n",
    "### 3.3 Rewards\n",
    "\n",
    "* win the game +10 points.\n",
    "* lose the game -15 points. \n",
    "* tie +0 points. \n",
    "* reward for each step = weight1 * points_self_get + weight2 * points_block_rival_get.\n",
    "\n",
    "<a name=\"3.4\"></a>\n",
    "### 3.4 Game Termination\n",
    "\n",
    "* All 25 steps are taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4 - Make the Game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILVMYKewfR0n"
   },
   "outputs": [],
   "source": [
    "class five_tigers():\n",
    "\n",
    "    def __init__(self, initial=[[EMPTY for _ in range(board_size)] for _ in range(board_size)]):\n",
    "        \"\"\"\n",
    "        Initialize game board.\n",
    "        Each game board has\n",
    "            - `board`: a list of the playing board\n",
    "            - `player`: 0 or 1 to indicate which player's turn\n",
    "            - `winner`: None, 0, or 1 to indicate who the winner is\n",
    "        \"\"\"\n",
    "        self.board = copy.deepcopy(initial)\n",
    "        self.player = 0\n",
    "        self.winner = None\n",
    "        self.scores = [0, 0]\n",
    "        self.left = 25\n",
    "        self.translation = {X:1, O:-1, EMPTY:0}\n",
    "\n",
    "    @classmethod\n",
    "    def available_actions(cls, board):\n",
    "        \"\"\"\n",
    "        five_tigers.available_actions(board) takes a `board` list as input\n",
    "        and returns all of the available actions `(i, j)` in that state.\n",
    "\n",
    "        Action `(i, j)` represents the action of make a move in raw_i,column_j\n",
    "        \"\"\"\n",
    "        actions = set()\n",
    "        for i in range(5) :\n",
    "            for j in range(5) :\n",
    "                if board[i][j] == EMPTY :\n",
    "                    actions.add((i,j))\n",
    "        return actions\n",
    "\n",
    "    @classmethod\n",
    "    def other_player(cls, player):\n",
    "        \"\"\"\n",
    "        five_tigers.other_player(player) returns the player that is not\n",
    "        `player`. Assumes `player` is either 0 or 1.\n",
    "        \"\"\"\n",
    "        return 0 if player == 1 else 1\n",
    "\n",
    "    def switch_player(self):\n",
    "        \"\"\"\n",
    "        Switch the current player to the other player.\n",
    "        \"\"\"\n",
    "        self.player = five_tigers.other_player(self.player)\n",
    "\n",
    "    def now_player_sign(self, player):\n",
    "        if player == 0:\n",
    "            return X\n",
    "        else:\n",
    "            return O\n",
    "        \n",
    "    def update_score(self, action):\n",
    "        related = {\n",
    "            (0, 0): [1, 6, 11, 21, 30, 31, ],\n",
    "            (0, 1): [1, 7, 13, 22, 31, 32, ],\n",
    "            (0, 2): [1, 8, 17, 18, 21, 23, 32, 33, ],\n",
    "            (0, 3): [1, 9, 14, 22, 33, 34, ],\n",
    "            (0, 4): [1, 10, 12, 23, 30, 34, ],\n",
    "            (1, 0): [2, 6, 15, 24, 31, 35, ],\n",
    "            (1, 1): [2, 7, 11, 17, 21, 25, 31, 32, 35, 36, ],\n",
    "            (1, 2): [2, 8, 13, 14, 22, 24, 26, 32, 33, 36, 37, ],\n",
    "            (1, 3): [2, 9, 12, 18, 23, 25, 33, 34, 37, 38, ],\n",
    "            (1, 4): [2, 10, 16, 26, 34, 38, ],\n",
    "            (2, 0): [3, 6, 17, 20, 21, 27, 35, 39, ],\n",
    "            (2, 1): [3, 7, 14, 15, 22, 24, 28, 35, 36, 39, 40, ],\n",
    "            (2, 2): [3, 8, 11, 12, 21, 23, 25, 27, 29, 30, 36, 37, 40, 41, ],\n",
    "            (2, 3): [3, 9, 13, 16, 22, 26, 28, 37, 38, 41, 42, ],\n",
    "            (2, 4): [3, 10, 18, 19, 23, 29, 38, 42, ],\n",
    "            (3, 0): [4, 6, 14, 24, 39, 43, ],\n",
    "            (3, 1): [4, 7, 12, 20, 25, 27, 39, 40, 43, 44, ],\n",
    "            (3, 2): [4, 8, 15, 16, 24, 26, 28, 40, 41, 44, 45, ],\n",
    "            (3, 3): [4, 9, 11, 19, 25, 29, 41, 42, 45, 46, ],\n",
    "            (3, 4): [4, 10, 13, 26, 42, 46, ],\n",
    "            (4, 0): [5, 6, 12, 27, 30, 43, ],\n",
    "            (4, 1): [5, 7, 16, 28, 43, 44, ],\n",
    "            (4, 2): [5, 8, 19, 20, 27, 29, 44, 45, ],\n",
    "            (4, 3): [5, 9, 15, 28, 45, 46, ],\n",
    "            (4, 4): [5, 10, 11, 29, 30, 46, ]\n",
    "        }\n",
    "        tmp_score = [0, 0]\n",
    "        def add_score(base_sign, score, k):\n",
    "            if base_sign == X:\n",
    "                tmp_score[0] += score\n",
    "                if k:\n",
    "                    self.scores[0] += score\n",
    "            else:\n",
    "                tmp_score[1] += score\n",
    "                if k:\n",
    "                    self.scores[1] += score\n",
    "        def check_raw(base_sign, i, k):\n",
    "            if self.board[i][0] == base_sign and self.board[i][1] == base_sign and self.board[i][2] == base_sign and self.board[i][3] == base_sign and self.board[i][4] == base_sign:\n",
    "                add_score(base_sign,5,k)\n",
    "        def check_column(base_sign, i, k):\n",
    "            if self.board[0][i] == base_sign and self.board[1][i] == base_sign and self.board[2][i] == base_sign and self.board[3][i] == base_sign and self.board[4][i] == base_sign:\n",
    "                add_score(base_sign,5,k)\n",
    "        def check_5x(base_sign, i, k):\n",
    "            if i == 1 and self.board[2][2] == base_sign and self.board[0][0] == base_sign and self.board[1][1] == base_sign and self.board[3][3] == base_sign and self.board[4][4] == base_sign:\n",
    "                add_score(base_sign,5,k)\n",
    "            if i == 2 and self.board[2][2] == base_sign and self.board[0][4] == base_sign and self.board[1][3] == base_sign and self.board[3][1] == base_sign and self.board[4][0] == base_sign:\n",
    "                add_score(base_sign,5,k)\n",
    "        def check_4x(base_sign, i, k):\n",
    "            if i == 2 and self.board[0][3] == base_sign and self.board[1][2] == base_sign and self.board[2][1] == base_sign and self.board[3][0] == base_sign:\n",
    "                add_score(base_sign,4,k)\n",
    "            if i == 1 and self.board[0][1] == base_sign and self.board[1][2] == base_sign and self.board[2][3] == base_sign and self.board[3][4] == base_sign:\n",
    "                add_score(base_sign,4,k)\n",
    "            if i == 3 and self.board[4][3] == base_sign and self.board[3][2] == base_sign and self.board[2][1] == base_sign and self.board[1][0] == base_sign:\n",
    "                add_score(base_sign,4,k)\n",
    "            if i == 4 and self.board[4][1] == base_sign and self.board[3][2] == base_sign and self.board[2][3] == base_sign and self.board[1][4] == base_sign:\n",
    "                add_score(base_sign,4,k)\n",
    "        def check_3x(base_sign, i, k):\n",
    "            if i == 1 and self.board[0][2] == base_sign and self.board[1][1] == base_sign and self.board[2][0] == base_sign:\n",
    "                add_score(base_sign,3,k)\n",
    "            if i == 2 and self.board[0][2] == base_sign and self.board[1][3] == base_sign and self.board[2][4] == base_sign:\n",
    "                add_score(base_sign,3,k)\n",
    "            if i == 4 and self.board[4][2] == base_sign and self.board[3][1] == base_sign and self.board[2][0] == base_sign:\n",
    "                add_score(base_sign,3,k)\n",
    "            if i == 3 and self.board[4][2] == base_sign and self.board[3][3] == base_sign and self.board[2][4] == base_sign:\n",
    "                add_score(base_sign,3,k)\n",
    "        def check_big5(base_sign, k):\n",
    "            if self.board[2][2] == base_sign and self.board[0][0] == base_sign and self.board[4][0] == base_sign and self.board[0][4] == base_sign and self.board[4][4] == base_sign:\n",
    "                add_score(base_sign,10,k)\n",
    "        def check_small5(base_sign, index, k):\n",
    "            i, j = index // 3 + 1, index % 3 + 1\n",
    "            if self.board[i][j] == base_sign and self.board[i-1][j-1] == base_sign and self.board[i-1][j+1] == base_sign and self.board[i+1][j-1] == base_sign and self.board[i+1][j+1] == base_sign:\n",
    "                add_score(base_sign,5,k)\n",
    "        def check_well(base_sign, index, k):\n",
    "            i, j = index // 4, index % 4\n",
    "            if self.board[i][j] == base_sign and self.board[i][j+1] == base_sign and self.board[i+1][j] == base_sign and self.board[i+1][j+1] == base_sign:\n",
    "                add_score(base_sign,1,k)\n",
    "        \n",
    "        i, j = action  # tuple\n",
    "        base_sign = self.board[i][j]\n",
    "        for index in related[action]:\n",
    "            if 1 <= index <= 5:\n",
    "                check_raw(base_sign, index-1, 1)\n",
    "            elif 6 <= index <= 10:\n",
    "                check_column(base_sign, index-6, 1)\n",
    "            elif 11 <= index <= 12:\n",
    "                check_5x(base_sign, index-10, 1)\n",
    "            elif 13 <= index <= 16:\n",
    "                check_4x(base_sign, index-12, 1)\n",
    "            elif 17 <= index <= 20:\n",
    "                check_3x(base_sign, index-16, 1)\n",
    "            elif 21 <= index <= 29:\n",
    "                check_small5(base_sign, index-21, 1)\n",
    "            elif index == 30:\n",
    "                check_big5(base_sign, 1)\n",
    "            elif 31 <= index <= 46:\n",
    "                check_well(base_sign, index-31, 1)\n",
    "\n",
    "        self.board[i][j] = self.now_player_sign(1-self.player)\n",
    "        base_sign = self.board[i][j]\n",
    "        for index in related[action]:\n",
    "            if 1 <= index <= 5:\n",
    "                check_raw(base_sign, index-1, 0)\n",
    "            elif 6 <= index <= 10:\n",
    "                check_column(base_sign, index-6, 0)\n",
    "            elif 11 <= index <= 12:\n",
    "                check_5x(base_sign, index-10, 0)\n",
    "            elif 13 <= index <= 16:\n",
    "                check_4x(base_sign, index-12, 0)\n",
    "            elif 17 <= index <= 20:\n",
    "                check_3x(base_sign, index-16, 0)\n",
    "            elif 21 <= index <= 29:\n",
    "                check_small5(base_sign, index-21, 0)\n",
    "            elif index == 30:\n",
    "                check_big5(base_sign, 0)\n",
    "            elif 31 <= index <= 46:\n",
    "                check_well(base_sign, index-31, 0)\n",
    "\n",
    "        self.board[i][j] = self.now_player_sign(self.player)\n",
    "        index = int((self.translation[self.board[i][j]]+1)/2)\n",
    "        return 1.5*((1 - SELF_RACIO) * tmp_score[index] + SELF_RACIO * tmp_score[1- index]) - 0.1*(25-self.left)\n",
    "\n",
    "    def check_winner(self):\n",
    "        if self.scores[0] > self.scores[1]:\n",
    "            return 0\n",
    "        elif self.scores[0] < self.scores[1]:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "        \n",
    "    def board_to_state(self):\n",
    "        state = tuple([tuple([self.translation[j] for j in row])for row in self.board])\n",
    "        return state  # 2 dimension tuple\n",
    "        \n",
    "    def move(self, action):\n",
    "        \"\"\"\n",
    "        Make the move `action` for the current player.\n",
    "        `action` must be a tuple `(i, j)`.\n",
    "        \"\"\"\n",
    "        raw, column = action  # tuple must be valid move\n",
    "\n",
    "        # Check for errors\n",
    "        if self.winner is not None:\n",
    "            raise Exception(\"Game already won\")\n",
    "        elif raw < 0 or raw >= 5 or column < 0 or column >= 5:\n",
    "            raise Exception(\"Invalid move\")\n",
    "        \n",
    "        if self.board[raw][column] is not None:  # with our mask, this is not gonna happen \n",
    "            print(\"error:ai made an invalid move\")\n",
    "            return self.board_to_state(), 0, 1\n",
    "\n",
    "        # Update board\n",
    "        self.board[raw][column] = self.now_player_sign(self.player)\n",
    "        reward = self.update_score(action)\n",
    "        self.switch_player()\n",
    "        self.left -= 1\n",
    "\n",
    "        if self.left == 0:\n",
    "            self.winner = self.check_winner()\n",
    "            return self.board_to_state(), reward, 1\n",
    "        elif self.left == 1:\n",
    "            return self.board_to_state(), reward, 1\n",
    "        else:\n",
    "            return self.board_to_state(), reward, 0\n",
    "        \n",
    "    def render(self):\n",
    "        print()\n",
    "        print(\"board:\")\n",
    "        print(\"   0 1 2 3 4\")\n",
    "        for i in range(5):\n",
    "            print(i,end=\"  \")\n",
    "            for j in self.board[i]:\n",
    "                print(j if j is not None else '-',end=\" \")\n",
    "            print()\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build our neural network later on we need to know the size of the state vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5 - Interacting with the Game\n",
    "\n",
    "* play a step\n",
    "* use state and Q_net work to decide an action\n",
    "* last_state + 2 actions = new_state\n",
    "* reward is explained above\n",
    "* store (last_state, action, new_state, reward, done) pair in the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n",
    "## 6 - Deep Q-Learning\n",
    "\n",
    "In cases where both the state and action space are discrete we can estimate the action-value function iteratively by using the Bellman equation:\n",
    "\n",
    "$$\n",
    "Q_{i+1}(s,a) = R + \\gamma \\max_{a'}Q_i(s',a')\n",
    "$$\n",
    "\n",
    "This iterative method converges to the optimal action-value function $Q^*(s,a)$ as $i\\to\\infty$. This means that the agent just needs to gradually explore the state-action space and keep updating the estimate of $Q(s,a)$ until it converges to the optimal action-value function $Q^*(s,a)$. However, in cases where the state space is continuous it becomes practically impossible to explore the entire state-action space. Consequently, this also makes it practically impossible to gradually estimate $Q(s,a)$ until it converges to $Q^*(s,a)$.\n",
    "\n",
    "In the Deep $Q$-Learning, we solve this problem by using a neural network to estimate the action-value function $Q(s,a)\\approx Q^*(s,a)$. We call this neural network a $Q$-Network and it can be trained by adjusting its weights at each iteration to minimize the mean-squared error in the Bellman equation.\n",
    "\n",
    "Unfortunately, **using neural networks in reinforcement learning to estimate action-value functions has proven to be highly unstable**. Luckily, there's a couple of techniques that can be employed to avoid instabilities. These techniques consist of using a ***Target Network*** and ***Experience Replay***. We will explore these two techniques in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"6.1\"></a>\n",
    "### 6.1 Target Network\n",
    "\n",
    "We can train the $Q$-Network by adjusting it's weights at each iteration to minimize the mean-squared error in the Bellman equation, where the target values are given by:\n",
    "\n",
    "$$\n",
    "y = R + \\gamma \\max_{a'}Q(s',a';w)\n",
    "$$\n",
    "\n",
    "where $w$ are the weights of the $Q$-Network. This means that we are adjusting the weights $w$ at each iteration to minimize the following error:\n",
    "\n",
    "$$\n",
    "\\overbrace{\\underbrace{R + \\gamma \\max_{a'}Q(s',a'; w)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}}\n",
    "$$\n",
    "\n",
    "Notice that this forms a problem because the $y$ target is changing on every iteration. **Having a constantly moving target can lead to oscillations and instabilities**. To avoid this, we can create\n",
    "a separate neural network for generating the $y$ targets. We call this separate neural network the **target $\\hat Q$-Network** and it will have the same architecture as the original $Q$-Network. By using the target $\\hat Q$-Network, the above error becomes:\n",
    "\n",
    "$$\n",
    "\\overbrace{\\underbrace{R + \\gamma \\max_{a'}\\hat{Q}(s',a'; w^-)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}}\n",
    "$$\n",
    "\n",
    "where $w^-$ and $w$ are the weights the target $\\hat Q$-Network and $Q$-Network, respectively.\n",
    "\n",
    "In practice, we will use the following algorithm: every $C$ time steps we will use the $\\hat Q$-Network to generate the $y$ targets and update the weights of the target $\\hat Q$-Network using the weights of the $Q$-Network. We will update the weights $w^-$ of the the target $\\hat Q$-Network using a **soft update**. This means that we will update the weights $w^-$ using the following rule:\n",
    " \n",
    "$$\n",
    "w^-\\leftarrow \\tau w + (1 - \\tau) w^-\n",
    "$$\n",
    "\n",
    "where $\\tau\\ll 1$. By using the soft update, we are ensuring that the target values, $y$, change slowly, which greatly improves the stability of our learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Q-Network\n",
    "q_network = Sequential([\n",
    "    ### START CODE HERE ### \n",
    "    Input(state_size),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(action_size, activation='linear')\n",
    "    ### END CODE HERE ### \n",
    "    ])\n",
    "\n",
    "# Create the target Q^-Network\n",
    "target_q_network = Sequential([\n",
    "    ### START CODE HERE ### \n",
    "    Input(state_size),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(action_size, activation='linear')\n",
    "    ### END CODE HERE ###\n",
    "    ])\n",
    "\n",
    "### START CODE HERE ### \n",
    "optimizer = Adam(ALPHA)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"6.2\"></a>\n",
    "### 6.2 Experience Replay\n",
    "\n",
    "When an agent interacts with the environment, the states, actions, and rewards the agent experiences are sequential by nature. If the agent tries to learn from these consecutive experiences it can run into problems due to the strong correlations between them. To avoid this, we employ a technique known as **Experience Replay** to generate uncorrelated experiences for training our agent. Experience replay consists of storing the agent's experiences (i.e the states, actions, and rewards the agent receives) in a memory buffer and then sampling a random mini-batch of experiences from the buffer to do the learning. The experience tuples $(S_t, A_t, R_t, S_{t+1})$ will be added to the memory buffer at each time step as the agent interacts with the environment.\n",
    "\n",
    "For convenience, we will store the experiences as named tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using experience replay we avoid problematic correlations, oscillations and instabilities. In addition, experience replay also allows the agent to potentially use the same experience in multiple weight updates, which increases data efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"7\"></a>\n",
    "## 7 - Deep Q-Learning Algorithm with Experience Replay\n",
    "\n",
    "Now that we know all the techniques that we are going to use, we can put them togther to arrive at the Deep Q-Learning Algorithm With Experience Replay.\n",
    "<br>\n",
    "<br>\n",
    "<figure>\n",
    "  <img src = \"images/deep_q_algorithm.png\" width = 90% style = \"border: thin silver solid; padding: 0px\">\n",
    "      <figcaption style = \"text-align: center; font-style: italic\">Fig 3. Deep Q-Learning with Experience Replay.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_network(q_network, target_q_network):\n",
    "    for target_weights, q_net_weights in zip(target_q_network.weights, q_network.weights):\n",
    "        target_weights.assign(TAU * q_net_weights + (1.0 - TAU) * target_weights)\n",
    "        \n",
    "def get_mask(states):  #(64,25)\n",
    "    mask = tf.where(tf.equal(states, 1.0), -100.0, states)\n",
    "    mask = tf.where(tf.equal(mask, -1.0), -100.0, mask)\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    \"\"\" \n",
    "    Calculates the loss.\n",
    "    \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "      q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
    "      target_q_network: (tf.keras.Sequential) Karas model for predicting the targets\n",
    "          \n",
    "    Returns:\n",
    "      loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
    "            the y targets and the Q(s,a) values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack the mini-batch of experience tuples. all of them are tf.tensor\n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "\n",
    "    # Compute max Q^(s,a)\n",
    "    mask = get_mask(next_states)\n",
    "    masked_target_q = target_q_network(next_states) + mask\n",
    "    max_qsa = tf.reduce_max(masked_target_q, axis=1)  # (64,) tensor\n",
    "\n",
    "    # Set y = R if episode terminates, otherwise set y = R + γ max Q^(s,a).\n",
    "    ### START CODE HERE ### \n",
    "    y_targets = done_vals * rewards + (1 - done_vals) * (rewards + gamma * max_qsa)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Get the q_values\n",
    "    q_values = q_network(states)\n",
    "    q_values = tf.gather(q_values, actions, axis=1, batch_dims=1)  # (64,)tensor\n",
    "        \n",
    "    # Compute the loss\n",
    "    ### START CODE HERE ### \n",
    "    loss = MSE(q_values, y_targets) \n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"8\"></a>\n",
    "## 8 - Update the Network Weights\n",
    "\n",
    "We will use the `agent_learn` function below to implement lines ***12 -14*** of the algorithm outlined in [Fig 3](#7). The `agent_learn` function will update the weights of the $Q$ and target $\\hat Q$ networks using a custom training loop. Because we are using a custom training loop we need to retrieve the gradients via a `tf.GradientTape` instance, and then call `optimizer.apply_gradients()` to update the weights of our $Q$-Network. Note that we are also using the `@tf.function` decorator to increase performance. Without this decorator our training will take twice as long. If you would like to know more about how to increase performance with `@tf.function` take a look at the [TensorFlow documentation](https://www.tensorflow.org/guide/function).\n",
    "\n",
    "The last line of this function updates the weights of the target $\\hat Q$-Network using a [soft update](#6.1). If you want to know how this is implemented in code we encourage you to take a look at the `utils.update_target_network` function in the `utils` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q_network.save('q_model_128', save_format='tf')\n",
    "#target_q_network.save('target_q_model_128', save_format='tf')\n",
    "# ... 稍后加载模型 ...\n",
    "#q_network = tf.keras.models.load_model('q_model')\n",
    "#target_q_network = tf.keras.models.load_model('target_q_model')\n",
    "\n",
    "\n",
    "#optimizer = Adam(ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def agent_learn(experiences, gamma):\n",
    "    \"\"\"\n",
    "    Updates the weights of the Q networks.\n",
    "    \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(experiences, gamma, q_network, target_q_network)\n",
    "\n",
    "    # Get the gradients of the loss with respect to the weights.\n",
    "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "    \n",
    "    # Update the weights of the q_network.\n",
    "    optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "\n",
    "    # update the weights of target q_network\n",
    "    update_target_network(q_network, target_q_network)\n",
    "\n",
    "    return loss  # to report the loss history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"9\"></a>\n",
    "## 9 - Train the Agent\n",
    "\n",
    "We are now ready to train our agent to solve the Lunar Lander environment. In the cell below we will implement the algorithm in [Fig 3](#7) line by line (please note that we have included the same algorithm below for easy reference. This will prevent you from scrolling up and down the notebook):\n",
    "\n",
    "* **Line 1**: We initialize the `memory_buffer` with a capacity of $N =$ `MEMORY_SIZE`. Notice that we are using a `deque` as the data structure for our `memory_buffer`.\n",
    "\n",
    "\n",
    "* **Line 2**: We skip this line since we already initialized the `q_network` in [Exercise 1](#ex01).\n",
    "\n",
    "\n",
    "* **Line 3**: We initialize the `target_q_network` by setting its weights to be equal to those of the `q_network`.\n",
    "\n",
    "\n",
    "* **Line 4**: We start the outer loop. Notice that we have set $M =$ `num_episodes = 2000`. This number is reasonable because the agent should be able to solve the Lunar Lander environment in less than `2000` episodes using this notebook's default parameters.\n",
    "\n",
    "\n",
    "* **Line 5**: We use the `.reset()` method to reset the environment to the initial state and get the initial state.\n",
    "\n",
    "\n",
    "* **Line 6**: We start the inner loop. Notice that we have set $T =$ `max_num_timesteps = 1000`. This means that the episode will automatically terminate if the episode hasn't terminated after `1000` time steps.\n",
    "\n",
    "\n",
    "* **Line 7**: The agent observes the current `state` and chooses an `action` using an $\\epsilon$-greedy policy. Our agent starts out using a value of $\\epsilon =$ `epsilon = 1` which yields an $\\epsilon$-greedy policy that is equivalent to the equiprobable random policy. This means that at the beginning of our training, the agent is just going to take random actions regardless of the observed `state`. As training progresses we will decrease the value of $\\epsilon$ slowly towards a minimum value using a given $\\epsilon$-decay rate. We want this minimum value to be close to zero because a value of $\\epsilon = 0$ will yield an $\\epsilon$-greedy policy that is equivalent to the greedy policy. This means that towards the end of training, the agent will lean towards selecting the `action` that it believes (based on its past experiences) will maximize $Q(s,a)$. We will set the minimum $\\epsilon$ value to be `0.01` and not exactly 0 because we always want to keep a little bit of exploration during training. If you want to know how this is implemented in code we encourage you to take a look at the `utils.get_action` function in the `utils` module.\n",
    "\n",
    "\n",
    "* **Line 8**: We use the `.step()` method to take the given `action` in the environment and get the `reward` and the `next_state`. \n",
    "\n",
    "\n",
    "* **Line 9**: We store the `experience(state, action, reward, next_state, done)` tuple in our `memory_buffer`. Notice that we also store the `done` variable so that we can keep track of when an episode terminates. This allowed us to set the $y$ targets in [Exercise 2](#ex02).\n",
    "\n",
    "\n",
    "* **Line 10**: We check if the conditions are met to perform a learning update. We do this by using our custom `utils.check_update_conditions` function. This function checks if $C =$ `NUM_STEPS_FOR_UPDATE = 4` time steps have occured and if our `memory_buffer` has enough experience tuples to fill a mini-batch. For example, if the mini-batch size is `64`, then our `memory_buffer` should have at least `64` experience tuples in order to pass the latter condition. If the conditions are met, then the `utils.check_update_conditions` function will return a value of `True`, otherwise it will return a value of `False`.\n",
    "\n",
    "\n",
    "* **Lines 11 - 14**: If the `update` variable is `True` then we perform a learning update. The learning update consists of sampling a random mini-batch of experience tuples from our `memory_buffer`, setting the $y$ targets, performing gradient descent, and updating the weights of the networks. We will use the `agent_learn` function we defined in [Section 8](#8) to perform the latter 3.\n",
    "\n",
    "\n",
    "* **Line 15**: At the end of each iteration of the inner loop we set `next_state` as our new `state` so that the loop can start again from this new state. In addition, we check if the episode has reached a terminal state (i.e we check if `done = True`). If a terminal state has been reached, then we break out of the inner loop.\n",
    "\n",
    "\n",
    "* **Line 16**: At the end of each iteration of the outer loop we update the value of $\\epsilon$, and check if the environment has been solved. We consider that the environment has been solved if the agent receives an average of `200` points in the last `100` episodes. If the environment has not been solved we continue the outer loop and start a new episode.\n",
    "\n",
    "Finally, we wanted to note that we have included some extra variables to keep track of the total number of points the agent received in each episode. This will help us determine if the agent has solved the environment and it will also allow us to see how our agent performed during training. We also use the `time` module to measure how long the training takes. \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<figure>\n",
    "  <img src = \"images/deep_q_algorithm.png\" width = 90% style = \"border: thin silver solid; padding: 0px\">\n",
    "      <figcaption style = \"text-align: center; font-style: italic\">Fig 4. Deep Q-Learning with Experience Replay.</figcaption>\n",
    "</figure>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiences(memory_buffer):\n",
    "    experiences = random.sample(memory_buffer, k=MINIBATCH_SIZE)  # list\n",
    "    states = tf.convert_to_tensor(np.array([np.array(e.state).flatten() for e in experiences if e is not None]),dtype=tf.float32) # 64*25 tensor\n",
    "    actions = tf.convert_to_tensor(np.array([ALL_ACTIONS.index(e.action) for e in experiences if e is not None]), dtype=tf.int32) # (64,) tensor\n",
    "    rewards = tf.convert_to_tensor(np.array([e.reward for e in experiences if e is not None]), dtype=tf.float32) # (64,) tensor\n",
    "    next_states = tf.convert_to_tensor(np.array([np.array(e.next_state).flatten() for e in experiences if e is not None]),dtype=tf.float32) # 64*25 tensor\n",
    "    done_vals = tf.convert_to_tensor(np.array([e.done for e in experiences if e is not None]).astype(np.uint8),           # (64,) tensor\n",
    "                                     dtype=tf.float32)\n",
    "    return (states, actions, rewards, next_states, done_vals)\n",
    "\n",
    "\n",
    "def check_update_conditions(t, num_steps_upd, memory_buffer):\n",
    "    if t % num_steps_upd == 4 and len(memory_buffer) > MINIBATCH_SIZE:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def get_new_eps(epsilon):\n",
    "    return max(E_MIN, E_DECAY*epsilon)\n",
    "\n",
    "def get_action(q_values, state, epsilon=0):\n",
    "    state = tf.squeeze(state).numpy()  # (25,)\n",
    "    valid_actions = [i for i in range(25) if state[i] == 0]  # 0 is available\n",
    "    if random.random() > epsilon:\n",
    "        action_mask = tf.convert_to_tensor(np.array([0 if i in valid_actions else -100 for i in range(25)]),dtype=tf.float32)\n",
    "        q_values = q_values[0] + action_mask\n",
    "        return ALL_ACTIONS[tf.argmax(q_values).numpy()]\n",
    "    else:\n",
    "        return ALL_ACTIONS[random.choice(valid_actions)]\n",
    "\n",
    "def plot_history(reward_history, rolling_window=20, lower_limit=None,\n",
    "                 upper_limit=None, plot_rw=True, plot_rm=True):\n",
    "    \n",
    "    if lower_limit is None or upper_limit is None:\n",
    "        rh = reward_history\n",
    "        xs = [x for x in range(len(reward_history))]\n",
    "    else:\n",
    "        rh = reward_history[lower_limit:upper_limit]\n",
    "        xs = [x for x in range(lower_limit,upper_limit)]\n",
    "    \n",
    "    df = pd.DataFrame(rh)\n",
    "    rollingMean = df.rolling(rolling_window).mean()\n",
    "\n",
    "    plt.figure(figsize=(10,7), facecolor='white')\n",
    "    \n",
    "    if plot_rw:\n",
    "        plt.plot(xs, rh, linewidth=1, color='cyan')\n",
    "    if plot_rm:\n",
    "        plt.plot(xs, rollingMean, linewidth=2, color='magenta')\n",
    "\n",
    "    text_color = 'black'\n",
    "        \n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor('black')\n",
    "    plt.grid()\n",
    "#     plt.title(\"Total Point History\", color=text_color, fontsize=40)\n",
    "    plt.xlabel('Episode', color=text_color, fontsize=30)\n",
    "    plt.ylabel('Total Points', color=text_color, fontsize=30)\n",
    "    yNumFmt = mticker.StrMethodFormatter('{x:,}')\n",
    "    ax.yaxis.set_major_formatter(yNumFmt)\n",
    "    ax.tick_params(axis='x', colors=text_color)\n",
    "    ax.tick_params(axis='y', colors=text_color)\n",
    "    plt.show()\n",
    "\n",
    "def print_last_game(buffer):\n",
    "    for e in list(buffer)[-25:]:\n",
    "        for i in range(5):\n",
    "            print(f'{e.state[i]}   ->   {e.next_state[i]}')\n",
    "        print(f'action: {e.action}, reward:{e.reward}, done: {e.done}')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_input(state):\n",
    "    state_flat = np.array(state).flatten()\n",
    "    return tf.convert_to_tensor(state_flat, dtype=tf.float32)[tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "num_rounds = 50000\n",
    "\n",
    "total_loss_history = []\n",
    "\n",
    "epsilon = 1    # initial ε value for ε-greedy policy\n",
    "\n",
    "# Create a memory buffer D with capacity N\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "# Set the target network weights equal to the Q-Network weights\n",
    "target_q_network.set_weights(q_network.get_weights())\n",
    "\n",
    "for i in range(num_rounds):\n",
    "    \n",
    "    # Reset the environment to the initial state and get the initial state\n",
    "    game = five_tigers()\n",
    "    \n",
    "    for t in range(25):\n",
    "        \n",
    "        state = game.board_to_state() # 2 dimension tuple\n",
    "        # From the current state S choose an action A using an ε-greedy policy\n",
    "        # state needs to be the right shape for the q_network\n",
    "        input_qn = state_to_input(state) # (1,25) tf tensor\n",
    "        q_values = q_network(input_qn) # (1,25) tf tensor\n",
    "        action = get_action(q_values, input_qn, epsilon) # tuple(i, j)\n",
    "\n",
    "        # Make move\n",
    "        next_state, reward, done = game.move(action)\n",
    "\n",
    "        if t != 24:\n",
    "        # using target_q_network to get next state where rival'Q is largest\n",
    "            next_state_list = [list(row) for row in next_state]  # (5,5) list\n",
    "            input_qn = state_to_input(next_state)\n",
    "            mask = tf.where(tf.equal(input_qn, 1.0), -100.0, input_qn)\n",
    "            mask = tf.where(tf.equal(mask, -1.0), -100.0, mask)\n",
    "            masked_target_q = target_q_network(input_qn) + mask\n",
    "            r, c = ALL_ACTIONS[tf.reshape(tf.argmax(masked_target_q, axis=1), [])]\n",
    "            update = 1 if game.player == 0 else -1\n",
    "            next_state_list[r][c] = update\n",
    "            next_state = tuple([tuple(row) for row in next_state_list])\n",
    "\n",
    "        # Store experience tuple (S,A,R,S') in the memory buffer.\n",
    "        # We store the done variable as well for convenience.\n",
    "        if t < 23:\n",
    "            memory_buffer.append(experience(state, action, reward, next_state, done))\n",
    "        elif t == 23:\n",
    "            O_state, O_action, O_reward, O_next_state, O_done = state, action, reward, next_state, done\n",
    "        else:\n",
    "            if game.winner == 1:\n",
    "                memory_buffer.append(experience(O_state, O_action, O_reward+10, O_next_state, O_done))\n",
    "                memory_buffer.append(experience(state, action, reward-15, next_state, done))\n",
    "            elif game.winner == 0:\n",
    "                memory_buffer.append(experience(O_state, O_action, O_reward-15, O_next_state, O_done))\n",
    "                memory_buffer.append(experience(state, action, reward+10, next_state, done))\n",
    "            else:\n",
    "                memory_buffer.append(experience(O_state, O_action, O_reward, O_next_state, O_done))\n",
    "                memory_buffer.append(experience(state, action, reward, next_state, done))\n",
    "        \n",
    "        # Only update the network every NUM_STEPS_FOR_UPDATE time steps.\n",
    "        update = check_update_conditions(t, NUM_STEPS_FOR_UPDATE, memory_buffer)\n",
    "        \n",
    "        if update:\n",
    "            # Sample random mini-batch of experience tuples (S,A,R,S') from Deque\n",
    "            experiences = get_experiences(memory_buffer)\n",
    "            \n",
    "            # Set the y targets, perform a gradient descent step,\n",
    "            # and update the network weights.\n",
    "            loss = agent_learn(experiences, GAMMA)\n",
    "            total_loss_history.append(loss)\n",
    "\n",
    "        \n",
    "    # Update the ε value\n",
    "    epsilon = get_new_eps(epsilon)\n",
    "    av_latest_loss = np.mean(total_loss_history[-1000:])\n",
    "    print(f\"\\rEpisode {i+1} | Total loss average: {av_latest_loss:.2f}; each score: {game.scores[0]}, {game.scores[1]}    \", end=\"\")\n",
    "    \n",
    "    if (i+1) % 1000 == 0:\n",
    "        print(f\"\\rEpisode {i+1} | Total loss average: {av_latest_loss:.2f}; epsilon: {epsilon}; loss: {total_loss_history[-1]}           \")\n",
    "    #if av_latest_loss > 3:\n",
    "    #    break \n",
    "\n",
    "        \n",
    "tot_time = time.time() - start\n",
    "\n",
    "print(f\"\\nTotal Runtime: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_last_game(memory_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the point history to see how our agent improved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_EUXxurfe8m",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the point history\n",
    "plot_history(total_loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_xwgaX5MnYt"
   },
   "source": [
    "<a name=\"10\"></a>\n",
    "## 10 - Play with the five-tigers AI\n",
    "\n",
    "Now that we have trained our agent, we can see it in action. We will use the `play` function to create a game with our agent using the trained $Q$-Network. We input a list of 0/1 to set the first/second player and how many games we play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(human_experiences, settings=[]):\n",
    "    \"\"\"\n",
    "    Play human game against the AI.\n",
    "    `settings` can be set 0 or 1 to specify whether\n",
    "    human player moves first or second.\n",
    "    a list with no elements means play one game with ai in random other\n",
    "    we store the play data in human_experiences\n",
    "    \"\"\"\n",
    "    loop = 1 if len(settings) == 0 else len(settings)\n",
    "    # If no player order set, choose human's order randomly\n",
    "    for l in range(loop):\n",
    "        if len(settings) == 0:\n",
    "            human_player = random.randint(0, 1)\n",
    "        else:\n",
    "            human_player = settings[l]\n",
    "\n",
    "        # Create new game\n",
    "        game = five_tigers()\n",
    "\n",
    "        # Game loop\n",
    "        for t in range(25):\n",
    "\n",
    "            # Print contents of board\n",
    "            game.render()\n",
    "\n",
    "            # Compute available actions\n",
    "            available_actions = five_tigers.available_actions(game.board)  # tuple(i, j)\n",
    "            time.sleep(1)\n",
    "\n",
    "            state = game.board_to_state() # 2 dimension tuple\n",
    "\n",
    "            # Let human make a move\n",
    "            if game.player == human_player:\n",
    "                print(\"Your Turn\")\n",
    "                while True:\n",
    "                    try:\n",
    "                        row = int(input(\"Choose Row: \"))\n",
    "                        column = int(input(\"Choose Column: \"))\n",
    "                        if (row, column) in available_actions:\n",
    "                            action = (row, column)\n",
    "                            break\n",
    "                        print(\"Invalid move, try again.\")\n",
    "                    except:\n",
    "                        print(\"Invalid move, try again.\")\n",
    "\n",
    "\n",
    "            # Have AI make a move\n",
    "            else:\n",
    "                print(\"AI's Turn\")\n",
    "                input_qn = state_to_input(state) # (1, 25) tf tensor\n",
    "                q_values = q_network(input_qn) # (1, 25) tf tensor\n",
    "                print(f'q values = {tf.reshape(tf.reduce_max(q_values, axis=1), [])}')\n",
    "                action = get_action(q_values, input_qn) # tuple(i, j)\n",
    "                row, column = action\n",
    "                print(f\"AI chose to move row {row}, column {column}.\")\n",
    "\n",
    "            # Make move\n",
    "            next_state, reward, done = game.move(action)\n",
    "\n",
    "            if t != 24:\n",
    "        # get next state where rival'Q is largest\n",
    "                next_state_list = [list(row) for row in next_state]  # (5,5) list\n",
    "                input_qn = state_to_input(next_state)\n",
    "                mask = tf.where(tf.equal(input_qn, 1.0), -100.0, input_qn)\n",
    "                mask = tf.where(tf.equal(mask, -1.0), -100.0, mask)\n",
    "                masked_target_q = target_q_network(input_qn) + mask\n",
    "                r, c = ALL_ACTIONS[tf.reshape(tf.argmax(masked_target_q, axis=1), [])]\n",
    "                update = 1 if game.player == 0 else -1\n",
    "                next_state_list[r][c] = update\n",
    "                next_state = tuple([tuple(row) for row in next_state_list])\n",
    "            # Store experience tuple (S,A,R,S') in the memory buffer.\n",
    "            # We store the done variable as well for convenience.\n",
    "            if t < 23:\n",
    "                memory_buffer.append(experience(state, action, reward, next_state, done))\n",
    "            elif t == 23:\n",
    "                O_state, O_action, O_reward, O_next_state, O_done = state, action, reward, next_state, done\n",
    "            else:\n",
    "                if game.winner == 1:\n",
    "                    memory_buffer.append(experience(O_state, O_action, O_reward+10, O_next_state, O_done))\n",
    "                    memory_buffer.append(experience(state, action, reward-15, next_state, done))\n",
    "                elif game.winner == 0:\n",
    "                    memory_buffer.append(experience(O_state, O_action, O_reward-15, O_next_state, O_done))\n",
    "                    memory_buffer.append(experience(state, action, reward+10, next_state, done))\n",
    "                else:\n",
    "                    memory_buffer.append(experience(O_state, O_action, O_reward, O_next_state, O_done))\n",
    "                    memory_buffer.append(experience(state, action, reward, next_state, done))\n",
    "\n",
    "            # Check for winner\n",
    "            if t == 24:\n",
    "                game.render()\n",
    "                print()\n",
    "                print(\"GAME OVER\")\n",
    "                if game.winner == human_player:\n",
    "                    print(f\"Winner is human\")\n",
    "                elif game.winner == 1-human_player:\n",
    "                    print(f\"Winner is AI\")\n",
    "                elif game.winner == 2:\n",
    "                    print(\"Tie\")\n",
    "                else:\n",
    "                    print(f\"Sorry, AI made an invalid move, you win\")\n",
    "                print('scores:')\n",
    "                print(f\"Human: {game.scores[human_player]}\")\n",
    "                print(f\"AI   : {game.scores[1-human_player]}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_experiences = deque(maxlen=MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Ttb_zLeJKiG"
   },
   "outputs": [],
   "source": [
    "play(human_experiences, settings=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take advantage of the symmetry of the board, we can expand our data 8-fold\n",
    "def expend_experiences(human_experiences):\n",
    "    def point_rotate(action, k):\n",
    "        r, c = action[0]-2, action[1]-2\n",
    "        for i in range(k):\n",
    "            r, c = -c, r\n",
    "        return (r+2, c+2)\n",
    "    def point_flip(action):\n",
    "        r, c = action\n",
    "        return (r, 4-c)\n",
    "    expended_human_experiences = deque(maxlen=MEMORY_SIZE)\n",
    "    for experiences in human_experiences:\n",
    "        expended_human_experiences.append(experiences)\n",
    "        state = np.array(experiences.state)\n",
    "        action = experiences.action\n",
    "        reward = experiences.reward\n",
    "        next_state = np.array(experiences.next_state)\n",
    "        done_val = experiences.done\n",
    "        flip_state = np.fliplr(state)\n",
    "        flip_action = point_flip(action)\n",
    "        flip_next_state = np.fliplr(next_state)\n",
    "        expended_human_experiences.append(experience(tuple(map(tuple, flip_state.tolist())), flip_action, reward, tuple(map(tuple, flip_next_state.tolist())), done_val))\n",
    "        for k in range(1,4):\n",
    "            expended_human_experiences.append(experience(tuple(map(tuple, np.rot90(state,k).tolist())),point_rotate(action,k), reward, tuple(map(tuple, np.rot90(next_state,k).tolist())), done_val))\n",
    "            expended_human_experiences.append(experience(tuple(map(tuple, np.rot90(flip_state,k).tolist())),point_rotate(flip_action,k), reward, tuple(map(tuple, np.rot90(flip_next_state,k).tolist())), done_val))\n",
    "    return expended_human_experiences\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using human_experiences to improve ai performance\n",
    "# sadly leading to overfitting\n",
    "if len(human_experiences) > MINIBATCH_SIZE:\n",
    "    start = time.time()\n",
    "\n",
    "    num_rounds = 250\n",
    "\n",
    "    total_loss_history = []\n",
    "\n",
    "    epsilon = 0.1    # initial ε value for ε-greedy policy\n",
    "\n",
    "    # Create a memory buffer D with capacity N\n",
    "\n",
    "    # Set the target network weights equal to the Q-Network weights\n",
    "    #target_q_network.set_weights(q_network.get_weights())\n",
    "\n",
    "    for i in range(num_rounds):\n",
    "        \n",
    "        buffer = expend_experiences(human_experiences)\n",
    "        # Sample random mini-batch of experience tuples (S,A,R,S') from Deque\n",
    "        experiences = get_experiences(buffer)\n",
    "        \n",
    "        # Set the y targets, perform a gradient descent step,\n",
    "        # and update the network weights.\n",
    "        loss = agent_learn(experiences, GAMMA)\n",
    "        total_loss_history.append(loss)\n",
    "\n",
    "        # Update the ε value\n",
    "        epsilon = get_new_eps(epsilon)\n",
    "        av_latest_loss = np.mean(total_loss_history[-100:])\n",
    "        print(f\"\\rEpisode {i+1} | Total loss average: {av_latest_loss:.2f}    \", end=\"\")\n",
    "\n",
    "        if (i+1) % 1000 == 0:\n",
    "            print(f\"\\rEpisode {i+1} | Total loss average: {av_latest_loss:.2f}; epsilon: {epsilon}                          \")\n",
    "\n",
    "            \n",
    "    tot_time = time.time() - start\n",
    "\n",
    "    print(f\"\\nTotal Runtime: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")\n",
    "else:\n",
    "    print('please play with AI more times to create enough training set')\n",
    "    print(f'now training set length: {len(human_experiences)}')\n",
    "    print('least enough training set length: 64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TensorFlow - Lunar Lander.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
