{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d66efc0",
   "metadata": {},
   "source": [
    "- guideline\n",
    "    - nodes correspond to states $s$\n",
    "    - edges refer to actions $a$\n",
    "        - each edge transfers the environment from its parent state to its child state\n",
    "            - state transition\n",
    "    - UCT => pUCT: Q + U \n",
    "        - early on the simulation, U dominates (more exploration)\n",
    "        - but later, Q is more important (less exploration, more exploitation)\n",
    "    - training & inference\n",
    "        - training: uct = Q + U(select node)\n",
    "        - inference: visits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ad8d83a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T15:10:25.417902Z",
     "start_time": "2024-03-19T15:10:25.226000Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import math\n",
    "from IPython.display import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from copy import deepcopy\n",
    "import time\n",
    "from collections import deque\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "512dc845",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_puct = 2\n",
    "BOARD_SIZE = 5\n",
    "ALL_ACTIONS = [(r, c) for r in range(BOARD_SIZE) for c in range(BOARD_SIZE)]\n",
    "X, O, EMPTY = 'X', 'O', 0\n",
    "iterations = 600\n",
    "temp = 1e-3\n",
    "MEMORY_SIZE = 10000\n",
    "dirichlet = 0.3\n",
    "MINIBATCH_SIZE = 256\n",
    "input_shape = (5,5,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b194f9",
   "metadata": {},
   "source": [
    "## Node & search "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e4c7b",
   "metadata": {},
   "source": [
    "### Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ab0680a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T15:10:25.432755Z",
     "start_time": "2024-03-19T15:10:25.420388Z"
    }
   },
   "outputs": [],
   "source": [
    "class UCTNode():\n",
    "    def __init__(self, state, action, parent=None):\n",
    "\n",
    "        self.state = state  # Board() class\n",
    "        self.action = action  # int\n",
    "        \n",
    "        self.is_expanded = False\n",
    "        if state.left == 0:\n",
    "            self.is_terminal = True\n",
    "        else:\n",
    "            self.is_terminal = False\n",
    "\n",
    "        self.parent = parent  # UCTNode\n",
    "        \n",
    "        self.children = {}  # Dict[action, UCTNode]\n",
    "        # p\n",
    "        self.child_priors = np.zeros([25], dtype=np.float32)\n",
    "        # ti\n",
    "        self.child_total_value = np.zeros([25], dtype=np.float32)\n",
    "        # ni\n",
    "        self.child_number_visits = np.zeros([25], dtype=np.float32)\n",
    "    \n",
    "    \n",
    "    # Ni\n",
    "    @property\n",
    "    def number_visits(self):\n",
    "        return self.parent.child_number_visits[self.action]\n",
    "\n",
    "    @number_visits.setter\n",
    "    def number_visits(self, value):\n",
    "        self.parent.child_number_visits[self.action] = value\n",
    "        \n",
    "    # ti\n",
    "    @property\n",
    "    def total_value(self):\n",
    "        return self.parent.child_total_value[self.action]\n",
    "\n",
    "    @total_value.setter\n",
    "    def total_value(self, value):\n",
    "        self.parent.child_total_value[self.action] = value\n",
    "\n",
    "    # pUCT\n",
    "    def child_Q(self) -> np.ndarray:\n",
    "        return self.child_total_value / (1 + self.child_number_visits)\n",
    "\n",
    "\n",
    "    def child_U(self) -> np.ndarray:\n",
    "        return c_puct * math.sqrt(self.number_visits) * (\n",
    "            self.child_priors / (1 + self.child_number_visits))\n",
    "    \n",
    "    \n",
    "    def best_child(self) -> int:\n",
    "        return np.argmax(self.child_Q() + self.child_U())\n",
    "    \n",
    "    # traversal\n",
    "    # using pUCT to select a leaf that is terminal or not expanded\n",
    "    def select_leaf(self):  \n",
    "        current = self\n",
    "        while current.is_expanded and not current.is_terminal:\n",
    "            # pUCT\n",
    "            best_action = current.best_child()\n",
    "            current = current.maybe_add_child(best_action)\n",
    "        return current\n",
    "    \n",
    "    # if leaf not terminal, mark it expanded and give it priors to exclude invalid moves\n",
    "    def expand(self, child_priors):\n",
    "        self.is_expanded = True\n",
    "        if not self.is_terminal:\n",
    "            self.child_priors = child_priors\n",
    "            for action in range(25):\n",
    "                if ALL_ACTIONS[action] not in self.state.generate_actions():\n",
    "                    self.child_priors[action] = 0\n",
    "                    self.child_total_value[action] = -100\n",
    "\n",
    "    # expanded leaf should have all valid children, if not, add one\n",
    "    def maybe_add_child(self, action):\n",
    "        if action not in self.children:\n",
    "            # add next state UCTNode into children\n",
    "            self.children[action] = UCTNode(\n",
    "                self.state.move(ALL_ACTIONS[action]), action, parent=self)\n",
    "        return self.children[action]\n",
    "    \n",
    "    # update visits and value to all node along the search tree\n",
    "    def backup(self, value_estimate: float):\n",
    "        current = self\n",
    "        while current.parent is not None:\n",
    "            current.number_visits += 1\n",
    "            current.total_value += value_estimate\n",
    "            if value_estimate != 0.5:  # not tie\n",
    "                value_estimate = -value_estimate\n",
    "            current = current.parent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a8c9f3",
   "metadata": {},
   "source": [
    "### Q + U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b7a668",
   "metadata": {},
   "source": [
    "- Ranking = Quality + Uncertainty (Q + U)\n",
    "    - Quality: exploitation\n",
    "    - Uncertainty: exploration\n",
    "        - FOMO（fear of missing out）\n",
    "        - P from policy network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55a2d29",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "&Q=\\frac{t_i}{1+n_i}\\\\\n",
    "&U=\\sqrt{\\ln N_i}\\times \\frac{P}{1+n_i}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b316be",
   "metadata": {},
   "source": [
    "## Game state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dde5187",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T15:10:25.473868Z",
     "start_time": "2024-03-19T15:10:25.466038Z"
    }
   },
   "outputs": [],
   "source": [
    "class Board():  # must contain (scores,player,board,valid actions,move)\n",
    "    # create constructor (init board class instance)\n",
    "    def __init__(self, board=[[EMPTY for _ in range(BOARD_SIZE)] for _ in range(BOARD_SIZE)]):\n",
    "\n",
    "        self.player = 1\n",
    "        self.board = deepcopy(board)\n",
    "        self.scores = [0, 0]\n",
    "        self.left = 25\n",
    "\n",
    "    def copy(self):\n",
    "        new = Board(self.board)\n",
    "        new.player = self.player\n",
    "        new.scores = self.scores.copy()\n",
    "        new.left = self.left\n",
    "        return new\n",
    "\n",
    "    def update_score(self, action):\n",
    "        related = {\n",
    "            (0, 0): [1, 6, 11, 21, 30, 31, ],\n",
    "            (0, 1): [1, 7, 13, 22, 31, 32, ],\n",
    "            (0, 2): [1, 8, 17, 18, 21, 23, 32, 33, ],\n",
    "            (0, 3): [1, 9, 14, 22, 33, 34, ],\n",
    "            (0, 4): [1, 10, 12, 23, 30, 34, ],\n",
    "            (1, 0): [2, 6, 15, 24, 31, 35, ],\n",
    "            (1, 1): [2, 7, 11, 17, 21, 25, 31, 32, 35, 36, ],\n",
    "            (1, 2): [2, 8, 13, 14, 22, 24, 26, 32, 33, 36, 37, ],\n",
    "            (1, 3): [2, 9, 12, 18, 23, 25, 33, 34, 37, 38, ],\n",
    "            (1, 4): [2, 10, 16, 26, 34, 38, ],\n",
    "            (2, 0): [3, 6, 17, 20, 21, 27, 35, 39, ],\n",
    "            (2, 1): [3, 7, 14, 15, 22, 24, 28, 35, 36, 39, 40, ],\n",
    "            (2, 2): [3, 8, 11, 12, 21, 23, 25, 27, 29, 30, 36, 37, 40, 41, ],\n",
    "            (2, 3): [3, 9, 13, 16, 22, 26, 28, 37, 38, 41, 42, ],\n",
    "            (2, 4): [3, 10, 18, 19, 23, 29, 38, 42, ],\n",
    "            (3, 0): [4, 6, 14, 24, 39, 43, ],\n",
    "            (3, 1): [4, 7, 12, 20, 25, 27, 39, 40, 43, 44, ],\n",
    "            (3, 2): [4, 8, 15, 16, 24, 26, 28, 40, 41, 44, 45, ],\n",
    "            (3, 3): [4, 9, 11, 19, 25, 29, 41, 42, 45, 46, ],\n",
    "            (3, 4): [4, 10, 13, 26, 42, 46, ],\n",
    "            (4, 0): [5, 6, 12, 27, 30, 43, ],\n",
    "            (4, 1): [5, 7, 16, 28, 43, 44, ],\n",
    "            (4, 2): [5, 8, 19, 20, 27, 29, 44, 45, ],\n",
    "            (4, 3): [5, 9, 15, 28, 45, 46, ],\n",
    "            (4, 4): [5, 10, 11, 29, 30, 46, ]\n",
    "        }\n",
    "        def add_score(base_sign, score):\n",
    "            if base_sign == 1:\n",
    "                    self.scores[0] += score\n",
    "            else:\n",
    "                    self.scores[1] += score\n",
    "        def check_raw(base_sign, i):\n",
    "            if self.board[i][0] == base_sign and self.board[i][1] == base_sign and self.board[i][2] == base_sign and self.board[i][3] == base_sign and self.board[i][4] == base_sign:\n",
    "                add_score(base_sign,5)\n",
    "        def check_column(base_sign, i):\n",
    "            if self.board[0][i] == base_sign and self.board[1][i] == base_sign and self.board[2][i] == base_sign and self.board[3][i] == base_sign and self.board[4][i] == base_sign:\n",
    "                add_score(base_sign,5)\n",
    "        def check_5x(base_sign, i):\n",
    "            if i == 1 and self.board[2][2] == base_sign and self.board[0][0] == base_sign and self.board[1][1] == base_sign and self.board[3][3] == base_sign and self.board[4][4] == base_sign:\n",
    "                add_score(base_sign,5)\n",
    "            if i == 2 and self.board[2][2] == base_sign and self.board[0][4] == base_sign and self.board[1][3] == base_sign and self.board[3][1] == base_sign and self.board[4][0] == base_sign:\n",
    "                add_score(base_sign,5)\n",
    "        def check_4x(base_sign, i):\n",
    "            if i == 2 and self.board[0][3] == base_sign and self.board[1][2] == base_sign and self.board[2][1] == base_sign and self.board[3][0] == base_sign:\n",
    "                add_score(base_sign,4)\n",
    "            if i == 1 and self.board[0][1] == base_sign and self.board[1][2] == base_sign and self.board[2][3] == base_sign and self.board[3][4] == base_sign:\n",
    "                add_score(base_sign,4)\n",
    "            if i == 3 and self.board[4][3] == base_sign and self.board[3][2] == base_sign and self.board[2][1] == base_sign and self.board[1][0] == base_sign:\n",
    "                add_score(base_sign,4)\n",
    "            if i == 4 and self.board[4][1] == base_sign and self.board[3][2] == base_sign and self.board[2][3] == base_sign and self.board[1][4] == base_sign:\n",
    "                add_score(base_sign,4)\n",
    "        def check_3x(base_sign, i):\n",
    "            if i == 1 and self.board[0][2] == base_sign and self.board[1][1] == base_sign and self.board[2][0] == base_sign:\n",
    "                add_score(base_sign,3)\n",
    "            if i == 2 and self.board[0][2] == base_sign and self.board[1][3] == base_sign and self.board[2][4] == base_sign:\n",
    "                add_score(base_sign,3)\n",
    "            if i == 4 and self.board[4][2] == base_sign and self.board[3][1] == base_sign and self.board[2][0] == base_sign:\n",
    "                add_score(base_sign,3)\n",
    "            if i == 3 and self.board[4][2] == base_sign and self.board[3][3] == base_sign and self.board[2][4] == base_sign:\n",
    "                add_score(base_sign,3)\n",
    "        def check_big5(base_sign):\n",
    "            if self.board[2][2] == base_sign and self.board[0][0] == base_sign and self.board[4][0] == base_sign and self.board[0][4] == base_sign and self.board[4][4] == base_sign:\n",
    "                add_score(base_sign,10)\n",
    "        def check_small5(base_sign, index):\n",
    "            i, j = index // 3 + 1, index % 3 + 1\n",
    "            if self.board[i][j] == base_sign and self.board[i-1][j-1] == base_sign and self.board[i-1][j+1] == base_sign and self.board[i+1][j-1] == base_sign and self.board[i+1][j+1] == base_sign:\n",
    "                add_score(base_sign,5)\n",
    "        def check_well(base_sign, index):\n",
    "            i, j = index // 4, index % 4\n",
    "            if self.board[i][j] == base_sign and self.board[i][j+1] == base_sign and self.board[i+1][j] == base_sign and self.board[i+1][j+1] == base_sign:\n",
    "                add_score(base_sign,1)\n",
    "        \n",
    "        i, j = action\n",
    "        base_sign = self.board[i][j]\n",
    "        for index in related[action]:\n",
    "            if 1 <= index <= 5:\n",
    "                check_raw(base_sign, index-1)\n",
    "            elif 6 <= index <= 10:\n",
    "                check_column(base_sign, index-6)\n",
    "            elif 11 <= index <= 12:\n",
    "                check_5x(base_sign, index-10)\n",
    "            elif 13 <= index <= 16:\n",
    "                check_4x(base_sign, index-12)\n",
    "            elif 17 <= index <= 20:\n",
    "                check_3x(base_sign, index-16)\n",
    "            elif 21 <= index <= 29:\n",
    "                check_small5(base_sign, index-21)\n",
    "            elif index == 30:\n",
    "                check_big5(base_sign)\n",
    "            elif 31 <= index <= 46:\n",
    "                check_well(base_sign, index-31)\n",
    "    \n",
    "    def check_winner(self):\n",
    "        if self.scores[0] > self.scores[1]:\n",
    "            return 1\n",
    "        elif self.scores[0] < self.scores[1]:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "            \n",
    "    # make move\n",
    "    def move(self, action):\n",
    "        row, col = action\n",
    "        # create new board instance that inherits from the current state\n",
    "        next_state = Board(self.board)\n",
    "        \n",
    "        # make move\n",
    "        next_state.board[row][col] = self.player\n",
    "        next_state.left = self.left - 1\n",
    "\n",
    "        # update scores\n",
    "        next_state.scores = self.scores.copy()\n",
    "        next_state.update_score(action)\n",
    "\n",
    "        # swap players\n",
    "        next_state.player = -self.player\n",
    "    \n",
    "        # return new board state\n",
    "        return next_state\n",
    "        \n",
    "    # generate legal moves to play in the current position\n",
    "    def generate_actions(self):\n",
    "        # define states list (move list - list of available actions to consider)\n",
    "        actions = []\n",
    "        \n",
    "        # loop over board rows\n",
    "        for row in range(BOARD_SIZE):\n",
    "            # loop over board columns\n",
    "            for col in range(BOARD_SIZE):\n",
    "                # make sure that current square is empty\n",
    "                if self.board[row][col] == EMPTY:\n",
    "                    # append available (row, col) action\n",
    "                    actions.append((row, col))\n",
    "        \n",
    "        # return the list of available actions (tuple)\n",
    "        return actions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0470ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fiver_tigers():\n",
    "    def __init__(self, board = Board()):\n",
    "        self.state = board\n",
    "        self.board = self.state.board\n",
    "        self.winner = None\n",
    "        self.player = self.state.player\n",
    "        self.left = self.state.left\n",
    "        self.scores = self.state.scores\n",
    "\n",
    "    def available_actions(self):\n",
    "        return self.state.generate_actions()\n",
    "    \n",
    "    def render(self):\n",
    "        print()\n",
    "        print(\"board:\")\n",
    "        print(\"   0 1 2 3 4\")\n",
    "        for i in range(BOARD_SIZE):\n",
    "            print(i,end=\"  \")\n",
    "            for j in self.board[i]:\n",
    "                print(j if j != 0 else '-',end=\" \")\n",
    "            print()\n",
    "        print()\n",
    "\n",
    "    def move(self, action):\n",
    "        self.state = self.state.move(action)\n",
    "        self.board = self.state.board\n",
    "        self.player = -self.player\n",
    "        self.left = self.state.left\n",
    "        self.scores = self.state.scores\n",
    "        if self.left == 0:\n",
    "            self.winner = self.state.check_winner() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a5cd13",
   "metadata": {},
   "source": [
    "## Policy network & Value network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4711c",
   "metadata": {},
   "source": [
    "- 结合使用策略网络（Policy network）来指导搜索方向, 并使用价值网络来评估棋局的潜在价值, 可以显著减少搜索树的大小，提高搜索的效率。\n",
    "    - 策略网络（Policy network）能够从先前的对局中学习到有效的走棋模式和策略，这相当于在搜索过程中加入了大量的“先验知识”（child_priors）。\n",
    "- 价值网络（value network）可以给出对当前棋局胜负的直接评估，而不需要到达游戏的终局。这种评估能力对于减少搜索深度、加速决策过程至关重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1e0e290",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T15:10:25.484653Z",
     "start_time": "2024-03-19T15:10:25.477567Z"
    }
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "# 公共层\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.Flatten())\n",
    "# 策略头（Policy Head）\n",
    "policy_head = layers.Dense(128, activation='relu')(model.output)\n",
    "policy_head = layers.Dense(25, activation='linear')(policy_head)  # 输出为动作概率(before softmax)\n",
    "\n",
    "# 价值头（Value Head）\n",
    "value_head = layers.Dense(128, activation='relu')(model.output)\n",
    "value_head = layers.Dense(1, activation='tanh')(value_head)  # 输出为状态值（理论范围在 -1 到 1）\n",
    "\n",
    "policy_value_network = tf.keras.Model(inputs=model.input, outputs=[policy_head, value_head])\n",
    "#policy_value_network.summary()\n",
    "#np.random.random([25]), np.random.random()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f6cb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#policy_value_network.save('policy_value_model', save_format='tf')\n",
    "\n",
    "# load\n",
    "policy_value_network = tf.keras.models.load_model('policy_value_model_cnn')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afec62d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network, experiences):\n",
    "    states, mcts_probs, mcts_values = experiences\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        policy_predictions, value_predictions = network(states)\n",
    "        \n",
    "        policy_loss = tf.keras.losses.categorical_crossentropy(mcts_probs, policy_predictions, from_logits=True)\n",
    "        policy_loss = tf.reduce_mean(policy_loss)\n",
    "        \n",
    "        value_loss = tf.keras.losses.mean_squared_error(mcts_values, value_predictions)\n",
    "        value_loss = tf.reduce_mean(value_loss)\n",
    "\n",
    "        loss = policy_loss + value_loss\n",
    "    \n",
    "    gradients = tape.gradient(loss, network.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, network.trainable_variables))\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b096dc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state2input(state):\n",
    "    state_array = np.array(state.board) #(5,5)\n",
    "    board_cnn_input = np.expand_dims(state_array, axis=-1) # (5,5,1)\n",
    "    board_cnn_input = np.expand_dims(board_cnn_input, axis=0) # (1,5,5,1)\n",
    "    #state_flat = np.array(state.board).flatten()  # (25,)\n",
    "    #return tf.convert_to_tensor(state_flat, dtype=tf.float32)[tf.newaxis, :]  # (1,25)\n",
    "    return board_cnn_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e368ba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiences(memory_buffer):\n",
    "    experiences = random.sample(memory_buffer, k=MINIBATCH_SIZE)  # list\n",
    "    states = tf.convert_to_tensor(np.array([np.expand_dims(np.array(e[0].board), axis=-1) for e in experiences if e is not None]),dtype=tf.float32)  # (n,5,5,1) tensor\n",
    "    #states = tf.convert_to_tensor(np.array([np.array(e[0].board).flatten() for e in experiences if e is not None]),dtype=tf.float32) # (n,25) tensor\n",
    "    mcts_probs = tf.convert_to_tensor(np.array([e[1] for e in experiences if e is not None]),dtype=tf.float32) # (n,25) tensor\n",
    "    mcts_values = tf.convert_to_tensor(np.array([[e[2]] for e in experiences if e is not None]), dtype=tf.float32) # (n,1) tensor\n",
    "\n",
    "    return (states, mcts_probs, mcts_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b45b9b",
   "metadata": {},
   "source": [
    "## UCT_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bfd089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):  \n",
    "    probs = np.exp(x - np.max(x))\n",
    "    probs /= np.sum(probs)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22302a5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T15:10:25.496061Z",
     "start_time": "2024-03-19T15:10:25.488658Z"
    }
   },
   "outputs": [],
   "source": [
    "class DummyNode(object):  # for root node\n",
    "    def __init__(self):\n",
    "        self.parent = None\n",
    "        self.child_total_value = collections.defaultdict(float)\n",
    "        self.child_number_visits = collections.defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8a29f5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T15:17:00.713981Z",
     "start_time": "2024-03-19T15:17:00.706368Z"
    }
   },
   "outputs": [],
   "source": [
    "def UCT_search(state, num_reads):\n",
    "    # root is current state need to select a best move\n",
    "    root = UCTNode(state, action=None, parent=DummyNode())\n",
    "    # repeated simuations for 'num_reads' times\n",
    "    for i in range(num_reads):\n",
    "        # start from root\n",
    "        leaf = root.select_leaf()\n",
    "        # child_priors: [0, 1]\n",
    "        child_priors, value_estimate = policy_value_network(state2input(leaf.state))\n",
    "        #child_priors, value_estimate =  np.random.random([25]), np.random.random()\n",
    "        child_priors = softmax(tf.squeeze(child_priors).numpy())\n",
    "        if leaf.state.left == 0:\n",
    "            winner = leaf.state.check_winner()\n",
    "            if winner == 0:\n",
    "                value_estimate = 0.5\n",
    "            elif winner == 1:\n",
    "                value_estimate = 1\n",
    "            else:\n",
    "                value_estimate = -1\n",
    "        else:\n",
    "            value_estimate = tf.squeeze(value_estimate).numpy().item()\n",
    "\n",
    "        leaf.expand(child_priors)\n",
    "        leaf.backup(value_estimate)\n",
    "    act_visits= [(action, root.child_number_visits[action]) for action in root.children]\n",
    "    acts, visits = zip(*act_visits)\n",
    "    act_probs = softmax(1.0 / temp * np.log(np.array(visits) + 1e-10))\n",
    "    return acts, act_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5cadd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "num_rounds = 500\n",
    "\n",
    "total_loss_history = []\n",
    "\n",
    "# Create a memory buffer D with capacity N\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "for i in range(num_rounds):\n",
    "    \n",
    "    # Reset the environment to the initial state and get the initial state\n",
    "    game = Fiver_tigers()\n",
    "    states, mcts_probs = [], []\n",
    "    \n",
    "    for t in range(25):\n",
    "        state = game.state # Board class\n",
    "        print(f'\\rEpisode {i+1}: now making move {t+1}, now score: {game.state.scores}', end='')\n",
    "        acts, act_probs = UCT_search(state, iterations)\n",
    "        move_probs = np.zeros(25)\n",
    "        move_probs[list(acts)] = act_probs\n",
    "        p=0.75*act_probs + 0.25*np.random.dirichlet(dirichlet * np.ones(len(act_probs)))\n",
    "        move = np.random.choice(\n",
    "            acts,\n",
    "            p=p/np.sum(p)\n",
    "        )\n",
    "\n",
    "        states.append(state)  # Board class\n",
    "        mcts_probs.append(move_probs)  # np array (25,)\n",
    "\n",
    "        # make actions\n",
    "        game.move(ALL_ACTIONS[move])\n",
    "        \n",
    "\n",
    "    winners = np.zeros(25)\n",
    "    if game.winner == -1:\n",
    "        winners[0::2] = 1\n",
    "        winners[1::2] = -1\n",
    "    elif game.winner == 1:\n",
    "        winners[0::2] = -1\n",
    "        winners[1::2] = 1\n",
    "    else:\n",
    "        winners += 0.5\n",
    "\n",
    "    memory_buffer.extend(zip(states, mcts_probs, winners))\n",
    "\n",
    "    if len(memory_buffer) > MINIBATCH_SIZE:\n",
    "        experiences = get_experiences(memory_buffer)\n",
    "        loss = train_network(policy_value_network, experiences)\n",
    "        total_loss_history.append(loss)\n",
    "\n",
    "    if len(total_loss_history) >= 1:\n",
    "        print(f\"\\rEpisode {i+1} | current loss: {np.mean(total_loss_history):.2f}\")\n",
    "    else:\n",
    "        print(f\"\\rEpisode {i+1} | haven't update yet\")\n",
    "\n",
    "    if (i+1) % 50 == 0:\n",
    "        policy_value_network.save('policy_value_model', save_format='tf')\n",
    "\n",
    "        \n",
    "tot_time = time.time() - start\n",
    "\n",
    "print(f\"\\nTotal Runtime: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27877832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import sys,os\n",
    "import time\n",
    "\n",
    "def play(human_experiences):\n",
    "    # 字体路径\n",
    "    FONT_PATH = './resources/OpenSans-Regular.ttf'\n",
    "\n",
    "    pygame.init()\n",
    "    size = width, height = 900, 600\n",
    "\n",
    "    # Colors\n",
    "    orange = (174, 130, 44)\n",
    "    black = (0, 0, 0)\n",
    "    white = (255, 255, 255)\n",
    "\n",
    "    screen = pygame.display.set_mode(size)\n",
    "\n",
    "    mediumFont = pygame.font.Font(FONT_PATH, 32)\n",
    "    largeFont = pygame.font.Font(FONT_PATH, 50)\n",
    "    scoreFont = pygame.font.Font(FONT_PATH, 40)\n",
    "\n",
    "    user = None\n",
    "    ai_turn = False\n",
    "    ai_move = 0\n",
    "    game = Fiver_tigers() \n",
    "    board = game.board\n",
    "    states, mcts_probs = [], []\n",
    "\n",
    "\n",
    "    while True:\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                sys.exit()\n",
    "\n",
    "        screen.fill(orange)\n",
    "\n",
    "        # Let user choose a player.\n",
    "        if user is None:\n",
    "\n",
    "            # Draw title\n",
    "            title = largeFont.render(\"Play Five-Tigers\", True, black)\n",
    "            titleRect = title.get_rect()\n",
    "            titleRect.center = ((width / 2), 60)\n",
    "            screen.blit(title, titleRect)\n",
    "\n",
    "            # Draw buttons\n",
    "            playXButton = pygame.Rect((width / 8), (height / 2), width / 4, 60)\n",
    "            playX = mediumFont.render(\"Play first\", True, orange)\n",
    "            playXRect = playX.get_rect()\n",
    "            playXRect.center = playXButton.center\n",
    "            pygame.draw.rect(screen, black, playXButton)\n",
    "            screen.blit(playX, playXRect)\n",
    "\n",
    "            playOButton = pygame.Rect(5 * (width / 8), (height / 2), width / 4, 60)\n",
    "            playO = mediumFont.render(\"Play second\", True, orange)\n",
    "            playORect = playO.get_rect()\n",
    "            playORect.center = playOButton.center\n",
    "            pygame.draw.rect(screen, black, playOButton)\n",
    "            screen.blit(playO, playORect)\n",
    "            \n",
    "            # Check if button is clicked\n",
    "            click, _, _ = pygame.mouse.get_pressed()\n",
    "            if click == 1:\n",
    "                mouse = pygame.mouse.get_pos()\n",
    "                if playXButton.collidepoint(mouse):\n",
    "                    time.sleep(0.2)\n",
    "                    user = 1\n",
    "                elif playOButton.collidepoint(mouse):\n",
    "                    time.sleep(0.2)\n",
    "                    user = -1\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Draw game board\n",
    "            tile_size = 80\n",
    "            tile_origin = (width / 2 - (2.5 * tile_size),\n",
    "                        height / 2 - (2.5 * tile_size))\n",
    "            line_origin = (width / 2 - (2 * tile_size),\n",
    "                        height / 2 - (2 * tile_size))\n",
    "            tiles = []\n",
    "            for i in range(5):\n",
    "                row = []\n",
    "                for j in range(5):\n",
    "                    rect = pygame.Rect(\n",
    "                        tile_origin[0] + j * tile_size,\n",
    "                        tile_origin[1] + i * tile_size,\n",
    "                        tile_size, tile_size\n",
    "                    )\n",
    "                    if i != 4 and j != 4:\n",
    "                        lines = pygame.Rect(\n",
    "                            line_origin[0] + j * tile_size,\n",
    "                            line_origin[1] + i * tile_size,\n",
    "                            tile_size, tile_size\n",
    "                        )\n",
    "                        pygame.draw.rect(screen, black, lines, 3)\n",
    "\n",
    "                    if board[i][j] != 0:\n",
    "                        if board[i][j] == 1:\n",
    "                            pygame.draw.circle(screen, black, rect.center, 30, 0)\n",
    "                        elif board[i][j] == -1:\n",
    "                            pygame.draw.circle(screen, white, rect.center, 30, 0)\n",
    "\n",
    "                    row.append(rect)\n",
    "                tiles.append(row)\n",
    "\n",
    "            game_over = game.left == 0\n",
    "            player = game.player\n",
    "\n",
    "            # Show title\n",
    "            if game_over:\n",
    "                winner = game.winner\n",
    "                if winner == 0:\n",
    "                    title = f\"Game Over: Tie.\"\n",
    "                else:\n",
    "                    if game.winner == user:\n",
    "                        title = f\"Game Over: Human wins.\"\n",
    "                    else:\n",
    "                        title = f\"Game Over: AI wins.\"\n",
    "            elif user == player:\n",
    "                if user == 1:\n",
    "                    title = f\"Play as black\"\n",
    "                elif user == -1:\n",
    "                    title = f\"Play as white\"\n",
    "            else:\n",
    "                title = f\"Computer thinking...\"\n",
    "            title = largeFont.render(title, True, black)\n",
    "            titleRect = title.get_rect()\n",
    "            titleRect.center = ((width / 2), 40)\n",
    "            screen.blit(title, titleRect)\n",
    "\n",
    "            # Check for AI move\n",
    "            if user != player and not game_over:\n",
    "                if ai_turn:\n",
    "                    state = game.state\n",
    "                    acts, act_probs = UCT_search(state, iterations)\n",
    "                    move_probs = np.zeros(25)\n",
    "                    move_probs[list(acts)] = act_probs\n",
    "                    p=act_probs\n",
    "                    move = np.random.choice(\n",
    "                        acts,\n",
    "                        p=p/np.sum(p)\n",
    "                    )\n",
    "\n",
    "                    states.append(state)  # Board class\n",
    "                    mcts_probs.append(move_probs)\n",
    "                    ai_move += 1\n",
    "                    # 更新根节点并重用搜索树\n",
    "                    game.move(ALL_ACTIONS[move])\n",
    "                    board = game.board\n",
    "                    ai_turn = False\n",
    "                else:\n",
    "                    ai_turn = True\n",
    "\n",
    "            # Check for a user move\n",
    "            click, _, _ = pygame.mouse.get_pressed()\n",
    "            if click == 1 and user == player and not game_over:\n",
    "                mouse = pygame.mouse.get_pos()\n",
    "                for i in range(5):\n",
    "                    for j in range(5):\n",
    "                        if (board[i][j] == 0 and tiles[i][j].collidepoint(mouse)):\n",
    "                            game.move((i,j))\n",
    "                            board = game.board\n",
    "\n",
    "            if game_over:\n",
    "                againButton = pygame.Rect(width / 3, height - 70, width / 3, 60)\n",
    "                again = mediumFont.render(\"Play Again\", True, orange)\n",
    "                againRect = again.get_rect()\n",
    "                againRect.center = againButton.center\n",
    "                pygame.draw.rect(screen, black, againButton)\n",
    "                screen.blit(again, againRect)\n",
    "\n",
    "                scores = scoreFont.render(\"scores:\", True, black)\n",
    "                scoresRect = scores.get_rect()\n",
    "                scoresRect.center = (100, 230)\n",
    "                scoresRect.x = 25\n",
    "                screen.blit(scores, scoresRect)\n",
    "\n",
    "                aiscores = scoreFont.render(f\"AI: {game.scores[1-int((1-user)/2)]}\", True, black)\n",
    "                aiscoresRect = aiscores.get_rect()\n",
    "                aiscoresRect.center = (100, 300)\n",
    "                aiscoresRect.x = 25\n",
    "                screen.blit(aiscores, aiscoresRect)\n",
    "\n",
    "                huscores = scoreFont.render(f'Human: {game.scores[int((1-user)/2)]}', True, black)\n",
    "                huscoresRect = huscores.get_rect()\n",
    "                huscoresRect.center = (100, 370)\n",
    "                huscoresRect.x = 25\n",
    "                screen.blit(huscores, huscoresRect)\n",
    "\n",
    "                if game.winner == user:\n",
    "                    ai_win = np.ones(ai_move)\n",
    "                elif game.winner == -user:\n",
    "                    ai_win = -np.ones(ai_move)\n",
    "                else:\n",
    "                    ai_win = np.zeros(ai_move)\n",
    "                human_experiences.extend(zip(states, mcts_probs, ai_win))\n",
    "\n",
    "                click, _, _ = pygame.mouse.get_pressed()\n",
    "                if click == 1:\n",
    "                    mouse = pygame.mouse.get_pos()\n",
    "                    if againButton.collidepoint(mouse):\n",
    "                        time.sleep(0.2)\n",
    "                        user = None\n",
    "                        game = Fiver_tigers()\n",
    "                        states, mcts_probs = [], []\n",
    "                        ai_move = 0\n",
    "                        board = game.board\n",
    "                        ai_turn = False\n",
    "\n",
    "        pygame.display.flip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb53dd00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T15:10:31.247196Z",
     "start_time": "2024-03-19T15:10:25.526002Z"
    }
   },
   "outputs": [],
   "source": [
    "human_experiences = deque(maxlen=MEMORY_SIZE)\n",
    "play(human_experiences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3f68df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expend_experiences(memory_buffer):\n",
    "    # copy Board class to create new (state, probs, values)\n",
    "    def copy_Board(BOARD):\n",
    "        new = Board(BOARD.board)\n",
    "        new.player = BOARD.player\n",
    "        new.scores = BOARD.scores.copy()\n",
    "        new.left = BOARD.left\n",
    "        return new\n",
    "        \n",
    "    expended_memory_buffer = deque(maxlen=MEMORY_SIZE*8)\n",
    "\n",
    "    for experiences in memory_buffer:\n",
    "        # original\n",
    "        expended_memory_buffer.append(experiences)  \n",
    "        state = np.array(experiences[0].board)\n",
    "        action_probs = experiences[1].reshape((5,5))\n",
    "        value = experiences[2]\n",
    "\n",
    "        # flip\n",
    "        flip_state = np.fliplr(state)\n",
    "        flip_action_probs = np.fliplr(action_probs)\n",
    "        flip_board = copy_Board(experiences[0])\n",
    "        flip_board.board = flip_state.tolist()\n",
    "        expended_memory_buffer.append((flip_board, flip_action_probs.reshape(25), value))\n",
    "\n",
    "        # rotate\n",
    "        for k in range(1,4):\n",
    "            rotate_board, rotate_flip_board = copy_Board(experiences[0]), copy_Board(experiences[0])\n",
    "            rotate_board.board, rotate_flip_board.board = np.rot90(state,k).tolist(), np.rot90(flip_state,k).tolist()\n",
    "            expended_memory_buffer.append((rotate_board, np.rot90(action_probs,k).reshape(25), value))\n",
    "            expended_memory_buffer.append((rotate_flip_board, np.rot90(flip_action_probs,k).reshape(25), value))\n",
    "    return expended_memory_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc62b41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_memory_buffer = expend_experiences(memory_buffer)\n",
    "print(len(extended_memory_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83e712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss_history = []\n",
    "for i in range(50000):\n",
    "    experiences = get_experiences(extended_memory_buffer)\n",
    "    loss = train_network(policy_value_network, experiences)\n",
    "    #policy_value_network.save('policy_value_model', save_format='tf')\n",
    "    total_loss_history.append(loss)\n",
    "    print(f\"\\rIteration {i+1} | current loss: {total_loss_history[-1]:.2f}\",end='')\n",
    "    if (i+1) % 1000 == 0:\n",
    "        av_latest_loss = np.mean(total_loss_history[-1000:])\n",
    "        print(f\"\\rIteration {i+1} | current loss: {av_latest_loss:.2f}\")\n",
    "    if (i+1) % 5000 == 0:\n",
    "        policy_value_network.save('policy_value_model_cnn', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9714c1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
